# -*- coding: utf-8 -*-
"""MLAssignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M5zyX-a_pZT-wPVeOyWF5Jr2iM7wnF-l
"""

#Machine learning-based offline signature verification systems

#install kaggle
!pip install -q kaggle

from google.colab import files
files.upload()

#CREATE A KAGGLE DIRECTORY
!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d robinreni/signature-verification-dataset

#!unzip signature-verification-dataset.zip

train_dir="../content/sign_data/train"
test_dir="../content/sign_data/test_data"

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import matplotlib.pyplot as plt
img = plt.imread('/content/sign_data/train/001/001_01.PNG')
plt.imshow(img)

img1 = plt.imread('/content/sign_data/train/001_forg/0119001_01.png')
plt.imshow(img1)

SIZE = 224

import cv2
import os
import glob

train_data = []
train_labels = []
#Training data
for per in os.listdir('/content/sign_data/train/'):
    for data in glob.glob('/content/sign_data/train/'+per+'/*.*'):
        img = cv2.imread(data)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (SIZE,SIZE))
        train_data.append([img])
        if per[-1]=='g':
            train_labels.append(np.array(1))
        else:
            train_labels.append(np.array(0))

train_data = np.array(train_data)/255.0
train_labels = np.array(train_labels)

#Testing Data

test_data = []
test_labels = []

for per in os.listdir('/content/sign_data/test/'):
    for data in glob.glob('/content/sign_data/test/'+per+'/*.*'):
        img = cv2.imread(data)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (SIZE,SIZE))
        test_data.append([img])
        if per[-1]=='g':
            test_labels.append(np.array(1))
        else:
            test_labels.append(np.array(0))

test_data = np.array(test_data)/255.0
test_labels = np.array(test_labels)

from keras.utils import to_categorical
train_labels = to_categorical(train_labels)

train_data.shape

train_data = train_data.reshape(-1, SIZE,SIZE, 3)
test_data = test_data.reshape(-1, SIZE,SIZE, 3)

train_data.shape

train_labels.shape

from sklearn.utils import shuffle
train_data,train_labels = shuffle(train_data,train_labels)
test_data,test_labels = shuffle(test_data,test_labels)

#Building a cnn architecture
from keras.models import Sequential, Model, load_model
from keras import applications
from keras import optimizers
from keras.layers import Dropout, Flatten, Dense
from keras.optimizers import Adam

base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))
base_model.summary()

add_model = Sequential()
add_model.add(Flatten(input_shape=base_model.output_shape[1:]))
add_model.add(Dense(256, activation='relu'))
add_model.add(Dense(2, activation='softmax'))

model = Model(inputs=base_model.input, outputs=add_model(base_model.output))
model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=1e-4),
              metrics=['accuracy'])

model.summary()

#Taking ideal epochs and batch size
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard
earlyStopping = EarlyStopping(monitor='val_loss',
                              min_delta=0,
                              patience=3,
                              verbose=1)

early_stop=[earlyStopping]


EPOCHS = 25
BS = 64
progess = model.fit(train_data,train_labels, batch_size=BS,epochs=EPOCHS, callbacks=early_stop,validation_split=.45)

#plotting training and validation accuracy graph
acc = progess.history['accuracy']
val_acc = progess.history['val_accuracy']
loss = progess.history['loss']
val_loss = progess.history['val_loss']
 
epochs = range(len(acc))
 
plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
 
plt.figure()

#plotting training and validation loss graph
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
 
plt.show()

#Using Test data to test the model 
pred = model.predict(test_data)

#determining accuracy
from sklearn.metrics import accuracy_score
accuracy_score(np.argmax(pred,axis=1), test_labels)

#determining recall score
from sklearn.metrics import recall_score
recall_score(np.argmax(pred,axis=1), test_labels)

#determining precision score
from sklearn.metrics import precision_score
precision_score(np.argmax(pred,axis=1), test_labels)

#determining f1 score
from sklearn.metrics import f1_score
f1_score(np.argmax(pred,axis=1), test_labels)